
# Learning_ML

------------------

# Resources

| Resources | Completion Status |
|----------|-------------------|
| Essence of Linear Algebra @3Blue1Brown | ✅ |
| Machine Learning Playlist @CampusX | ✅ |
| Hands-On Machine Learning with Scikit-Learn and TensorFlow |⏳|
| Intro to Deep Learning @MIT |⏳|
| Deep Learning Playlist @CampusX |⏳|
| Neural Networks @3Blue1Brown |⏳|
| Deep Learning for Coders with fastai & PyTorch @Oreilly |⏳|





## Progress

| Days  | Topics                                                                 | Resources |
|------:|------------------------------------------------------------------------|-----------|
| [Day00](Day00)   |Learning Python Libraries                   | [w3Schools](https://www.w3schools.com/python/default.asp) |
| [Day01](Day01)   | Basics of Linear Algebra                                              | [3blue1brown](https://www.youtube.com/@3blue1brown) |
| [Day02](Day02)   | Decomposition, Derivation, Integration, and Gradient Descent           | [3blue1brown](https://www.youtube.com/@3blue1brown) |
| [Day03](Day03)   | Supervised Learning, Regression and classification                     | [CampusX](https://www.youtube.com/watch?v=81ymPYEtFOw&t=498s)|
| [Day04](Day04)   | Unsupervised Learning: Clustering and dimensionality reduction         | [Machine Learning Specialization]|
| [Day05](Day05)   | Univariate Linear Regression                                           | [Machine Learning Specialization]|
| [Day17](Day17)   | Cost Functions                                                        | [Machine Learning Specialization]|
| [Day18](Day18)   | Gradient Descent                                                      | [CampusX](https://www.youtube.com/@campusx-official) |
| [Day19](Day19)   | Effect of learning Rate, Cost function and Data on GD                  | [CampusX](https://www.youtube.com/@campusx-official)|
| [Day20](Day20)   | Linear Regression with multiple features, Vectorization                | [Machine Learning Specialization]|
| [Day21](Day21) | Feature Scaling, Visualization of Multiple Regression and Polynomial Regression | [Machine Learning Specialization]|
| [Day22](Day22) | Feature Engineering, Polynomial Regression                             | [Machine Learning Specialization]|



-------------------------------------
## Day 00: Python Libraries
- Numpy
- Pandas
- Matplotlib
- str_Method
--------------------------------------

# Day 01: Basics of Linear Algebra

linear algebra is used to represent data, perform matrix operations, and solve equations in algorithms like regression, pca, and neural networks.

- Scalars, Vectors, Matrices, Tensors: Basic data structures for ML.

![Day 01: Basics of Linear Algebra](images/example_of_tensor.png)

- Linear Combination and Span: Representing data points as weighted sums. Used in Linear Regression and neural networks.

![Linear Combination and Span](images/3dlinear_transformation.png)

- Determinants: Matrix invertibility, unique solutions in linear regression.
- Dot and Cross Product: Similarity (e.g., in SVMs) and vector transformations.

Slow progress right?? but consistent wins the race!
-----------------------------------------------------

# Day 02:Decomposition, Derivation, Integration, and Gradient Descent
--------------------------------------
- Identity and Inverse Matrices: Solving equations (e.g., linear regression) and optimization (e.g., gradient descent).

- Eigenvalues and Eigenvectors: PCA, SVD, feature extraction; eigenvalues capture variance.

![Function and Gradient](images/function_gradient.png)

Notes Here

## Calculus Overview:
- Functions & Graphs: Relationship between input (e.g., house size) and output (e.g., house price).
![function and graphs](images/image.png)

- Derivatives: Adjust model parameters to minimize error in predictions (e.g., house price). 

- Partial Derivatives: Measure change with respect to one variable, used in neural networks for weight updates.

- Gradient Descent: Optimization to minimize the cost function (error).

- Optimization: Finding the best values (minima/maxima) of a function to improve predictions.

- Integrals: Calculate area under a curve, used in probabilistic models (e.g., Naive Bayes).
![Integrals](images/integration.png) 

Revised statistics and probability concepts. Ready for the ML Specialization course!
---------------------------------------------------------------------

# Day 03: Supervised Learning: Regression and Classification

- Supervised Learning:

![Supervised Learning](images/day03_supervisedlearning.gif)

- Classification and Regression:

![Classification and Regression ](images/day03_supervisedLearning.png)
------------------------------------------------------------------------

# Day 04: 


